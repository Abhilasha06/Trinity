{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [20/Oct/2019 02:23:03] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find faces\n",
      "[INFO] Found 1 Faces!\n",
      "[INFO] Image faces_detected.jpg written to filesystem:  True\n",
      "Matched: Abhilasha.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [20/Oct/2019 02:25:07] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [20/Oct/2019 02:26:47] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find faces\n",
      "[INFO] Found 2 Faces!\n",
      "[INFO] Image faces_detected.jpg written to filesystem:  True\n",
      "Matched: Abhilasha.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [20/Oct/2019 08:15:28] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: Rashi.jpg\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import render_template\n",
    "        \n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template(\"my_form.html\")\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def my_form_post():\n",
    "\n",
    "     \n",
    "    st=[]   \n",
    "    data = request.form['q']\n",
    "    print(data)\n",
    "    \n",
    "    if data == 'image captioning':\n",
    "        import numpy as np\n",
    "        from numpy import array\n",
    "        import pandas as pd\n",
    "        import pickle   \n",
    "        import string\n",
    "        import os\n",
    "        import cv2\n",
    "        from PIL import Image\n",
    "        import glob\n",
    "        from pickle import dump, load\n",
    "        from time import time\n",
    "        from keras.preprocessing import sequence\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                                Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "        from keras.optimizers import Adam, RMSprop\n",
    "        from keras.layers.wrappers import Bidirectional\n",
    "        from keras.layers.merge import add\n",
    "        from keras.applications.inception_v3 import InceptionV3\n",
    "        from keras.preprocessing import image\n",
    "        from keras.models import Model\n",
    "        from keras import Input, layers\n",
    "        from keras import optimizers\n",
    "        from keras.applications.inception_v3 import preprocess_input\n",
    "        from keras.preprocessing.text import Tokenizer\n",
    "        from keras.preprocessing.sequence import pad_sequences\n",
    "        from keras.utils import to_categorical\n",
    "        from keras.models import load_model\n",
    "        from keras.preprocessing.image import load_img\n",
    "        from keras.preprocessing.image import save_img\n",
    "        from keras.preprocessing.image import img_to_array\n",
    "        cam = cv2.VideoCapture(0) \n",
    "        while(cam.isOpened()): \n",
    "    \n",
    "            ret,frame = cam.read() \n",
    "            if ret==True: \n",
    "          \n",
    "                cv2.imwrite(\"C://Users//Abhilasha//Desktop//ppp.jpg\",frame)\n",
    "                cv2.imshow(\"frame\",frame)\n",
    "                break\n",
    "\n",
    "        cam.release() \n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        image = frame\n",
    "\n",
    "        def load_doc(filename):\n",
    "            file = open(filename, 'r')\n",
    "            text = file.read()\n",
    "            file.close()\n",
    "            return text\n",
    "\n",
    "        filename = \"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr8k.token.txt\"\n",
    "        doc = load_doc(filename)\n",
    "        def load_descriptions(doc):\n",
    "            mapping = dict()\n",
    "            for line in doc.split('\\n'):\n",
    "                tokens = line.split()\n",
    "                if len(line) < 2:\n",
    "                    continue\n",
    "                image_id, image_desc = tokens[0], tokens[1:]\n",
    "                image_id = image_id.split('.')[0]\n",
    "                image_desc = ' '.join(image_desc)\n",
    "                if image_id not in mapping:\n",
    "                    mapping[image_id] = list()\n",
    "                mapping[image_id].append(image_desc)\n",
    "            return mapping\n",
    "\n",
    "        descriptions = load_descriptions(doc)\n",
    "        def clean_descriptions(descriptions):\n",
    "            table = str.maketrans('', '', string.punctuation)\n",
    "            for key, desc_list in descriptions.items():\n",
    "                for i in range(len(desc_list)):\n",
    "                    desc = desc_list[i]\n",
    "                    desc = desc.split()\n",
    "                    desc = [word.lower() for word in desc]\n",
    "                    desc = [w.translate(table) for w in desc]\n",
    "                    desc = [word for word in desc if len(word)>1]\n",
    "                    desc = [word for word in desc if word.isalpha()]\n",
    "                    desc_list[i] =  ' '.join(desc)\n",
    "\n",
    "        clean_descriptions(descriptions)\n",
    "        def to_vocabulary(descriptions):\n",
    "            all_desc = set()\n",
    "            for key in descriptions.keys():\n",
    "                [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "            return all_desc\n",
    "\n",
    "        vocabulary = to_vocabulary(descriptions)\n",
    "        def save_descriptions(descriptions, filename):\n",
    "            lines = list()\n",
    "            for key, desc_list in descriptions.items():\n",
    "                for desc in desc_list:\n",
    "                    lines.append(key + ' ' + desc)\n",
    "            data = '\\n'.join(lines)\n",
    "            file = open(filename, 'w')\n",
    "            file.write(data)\n",
    "            file.close()\n",
    "\n",
    "        save_descriptions(descriptions, 'descriptions.txt')\n",
    "        def load_set(filename):\n",
    "            doc = load_doc(filename)\n",
    "            dataset = list()\n",
    "            for line in doc.split('\\n'):\n",
    "                if len(line) < 1:\n",
    "                    continue\n",
    "                identifier = line.split('.')[0]\n",
    "                dataset.append(identifier)\n",
    "            return set(dataset)\n",
    "\n",
    "        filename = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr_8k.trainImages.txt'\n",
    "        train = load_set(filename)\n",
    "        images = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Images\\\\'\n",
    "        img = glob.glob(images + '*.jpg')\n",
    "        train_images_file = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr_8k.trainImages.txt'\n",
    "        train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "\n",
    "        train_img = []\n",
    "\n",
    "        for i in img:\n",
    "            if i[len(images):] in train_images: \n",
    "                train_img.append(i) \n",
    "        test_images_file = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr_8k.testImages.txt'\n",
    "        test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "        test_img = []\n",
    "\n",
    "        for i in img: \n",
    "            if i[len(images):] in test_images: \n",
    "                test_img.append(i) \n",
    "        def load_clean_descriptions(filename, dataset):\n",
    "            doc = load_doc(filename)\n",
    "            descriptions = dict()\n",
    "            for line in doc.split('\\n'):\n",
    "                tokens = line.split()\n",
    "                image_id, image_desc = tokens[0], tokens[1:]\n",
    "                if image_id in dataset:\n",
    "                    if image_id not in descriptions:\n",
    "                        descriptions[image_id] = list()\n",
    "                    desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "                    descriptions[image_id].append(desc)\n",
    "            return descriptions\n",
    "\n",
    "        train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "        def preprocess(image_path):\n",
    "            img = load_img(image_path, target_size=(299, 299))\n",
    "            x = img_to_array(img)\n",
    "            x = np.expand_dims(x, axis=0)\n",
    "            x = preprocess_input(x)\n",
    "            return x\n",
    "        model = InceptionV3(weights='imagenet')\n",
    "        model_new = Model(model.input, model.layers[-2].output)\n",
    "        def encode(image):\n",
    "            image = preprocess(image) \n",
    "            fea_vec = model_new.predict(image) \n",
    "            fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) \n",
    "            return fea_vec\n",
    "        train_features = load(open(\"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\pickle\\\\encoded_train_images.pkl\", \"rb\"))\n",
    "\n",
    "        all_train_captions = []\n",
    "        for key, val in train_descriptions.items():\n",
    "            for cap in val:\n",
    "                all_train_captions.append(cap)\n",
    "\n",
    "        word_count_threshold = 10\n",
    "        word_counts = {}\n",
    "        nsents = 0\n",
    "        for sent in all_train_captions:\n",
    "            nsents += 1\n",
    "            for w in sent.split(' '):\n",
    "                word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "        vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "        ixtoword = {}\n",
    "        wordtoix = {}\n",
    "\n",
    "        ix = 1\n",
    "        for w in vocab:\n",
    "            wordtoix[w] = ix\n",
    "            ixtoword[ix] = w\n",
    "            ix += 1\n",
    "        vocab_size = len(ixtoword) + 1\n",
    "        def to_lines(descriptions):\n",
    "            all_desc = list()\n",
    "            for key in descriptions.keys():\n",
    "                [all_desc.append(d) for d in descriptions[key]]\n",
    "            return all_desc\n",
    "\n",
    "        def max_length(descriptions):\n",
    "            lines = to_lines(descriptions)\n",
    "            return max(len(d.split()) for d in lines)\n",
    "        max_length = max_length(train_descriptions)\n",
    "        def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "            X1, X2, y = list(), list(), list()\n",
    "            n=0\n",
    "            while 1:\n",
    "                for key, desc_list in descriptions.items():\n",
    "                    n+=1\n",
    "                    photo = photos[key+'.jpg']\n",
    "                    for desc in desc_list:\n",
    "                        seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                        for i in range(1, len(seq)):\n",
    "                            in_seq, out_seq = seq[:i], seq[i]\n",
    "                            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                            X1.append(photo)\n",
    "                            X2.append(in_seq)\n",
    "                            y.append(out_seq)\n",
    "                    if n==num_photos_per_batch:\n",
    "                        yield [[array(X1), array(X2)], array(y)]\n",
    "                        X1, X2, y = list(), list(), list()\n",
    "                        n=0\n",
    "        glove_dir = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\glove6b200d'\n",
    "        embeddings_index = {} \n",
    "        f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        f.close()\n",
    "    \n",
    "        embedding_dim = 200\n",
    "        embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "        for word, i in wordtoix.items():\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "        model=load_model('C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\model_weights\\\\model_39.h5')\n",
    "        image_p = \"C://Users//Abhilasha//Desktop//ppp.jpg\"\n",
    "   \n",
    "        start = time()\n",
    "        encoding_test = {}\n",
    "        encoding_test[image_p[len(images):]] = encode(image_p)\n",
    "        print(\"Time taken in seconds =\", time()-start)\n",
    "        with open(\"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\pickle\\\\encoded_test_image_single.pkl\", \"wb\") as encoded_pickle_single:\n",
    "            pickle.dump(encoding_test, encoded_pickle_single)\n",
    "        with open(\"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\pickle\\\\encoded_test_image_single.pkl\", \"rb\") as encoded_pickle_single:\n",
    "            encoding_test_single = load(encoded_pickle_single)\n",
    "        def greedySearch(photo):\n",
    "            in_text = 'startseq'\n",
    "            for i in range(max_length):\n",
    "                sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "                sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "                yhat = model.predict([photo,sequence], verbose=0)\n",
    "                yhat = np.argmax(yhat)\n",
    "                word = ixtoword[yhat]\n",
    "                in_text += ' ' + word\n",
    "                if word == 'endseq':\n",
    "                    break\n",
    "            final = in_text.split()\n",
    "            final = final[1:-1]\n",
    "            final = ' '.join(final)\n",
    "            return final\n",
    "    \n",
    "        for i in range (0,1):\n",
    "            pic = list(encoding_test.keys())[i]\n",
    "            image = encoding_test[pic].reshape((1,2048))\n",
    "\n",
    "            ll=greedySearch(image)\n",
    "   \n",
    "            return render_template('index.html', message=ll)\n",
    "    \n",
    "    elif data == 'find faces':\n",
    "        import os\n",
    "        import face_recognition\n",
    "        import cv2\n",
    "        cam = cv2.VideoCapture(0) \n",
    "        while(cam.isOpened()): \n",
    "    \n",
    "            ret,frame = cam.read() \n",
    "            if ret==True: \n",
    "          \n",
    "                cv2.imwrite(\"C://Users//Abhilasha//Desktop//ppp.jpg\",frame)\n",
    "                cv2.imshow(\"frame\",frame)\n",
    "                break\n",
    "\n",
    "        cam.release() \n",
    "        cv2.destroyAllWindows()\n",
    "        img = frame\n",
    "\n",
    "        faceCascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "        faces = faceCascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=3, minSize=(30, 30))\n",
    "\n",
    "        print(\"[INFO] Found {0} Faces!\".format(len(faces)))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        status = cv2.imwrite('C:\\\\Users\\\\Abhilasha\\\\faces_detected.jpg', img)\n",
    "        print(\"[INFO] Image faces_detected.jpg written to filesystem: \", status)\n",
    "        images = os.listdir('E://my_faces')\n",
    "\n",
    "        for (x, y, w, h)  in faces:\n",
    "\n",
    "            frame = img[y:y+h,x:x+w]\n",
    "\n",
    "            cv2.imwrite(\"C://Users//Abhilasha//st.jpg\", frame)\n",
    "            image_to_be_matched = face_recognition.load_image_file(\"C://Users//Abhilasha//st.jpg\")\n",
    "            encodings = face_recognition.face_encodings(image_to_be_matched, num_jitters=100)\n",
    "            if len(encodings) > 0:\n",
    "                image_to_be_matched_encoded = encodings[0]\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            for image in images:\n",
    "   \n",
    "                current_image = face_recognition.load_image_file(\"E://my_faces//\" + image)\n",
    "                current_image_encoded = face_recognition.face_encodings(current_image)[0]\n",
    "                result = face_recognition.compare_faces([image_to_be_matched_encoded], current_image_encoded, tolerance = 0.5)\n",
    "                if result[0] == True:\n",
    "                    print(\"Matched: \"+image)\n",
    "                    st.append(image.split('.')[0])\n",
    "  \n",
    "        list_1_string = ' '.join(st)\n",
    "        list_1_string = \"Found \" + list_1_string\n",
    "    \n",
    "        return render_template('index.html', message=list_1_string)\n",
    "    \n",
    "    elif data == 'text detection':\n",
    "        import numpy as np\n",
    "        import cv2\n",
    "        import re\n",
    "        from textblob import TextBlob\n",
    "        from imutils.object_detection import non_max_suppression\n",
    "        import pytesseract\n",
    "        cam = cv2.VideoCapture(0) \n",
    "        while(cam.isOpened()): \n",
    "    \n",
    "            ret,frame = cam.read() \n",
    "            if ret==True: \n",
    "          \n",
    "                cv2.imwrite(\"C://Users//Abhilasha//Desktop//ppp.jpg\",frame)\n",
    "                cv2.imshow(\"frame\",frame)\n",
    "                break\n",
    "\n",
    "\n",
    "        cam.release() \n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        image = frame\n",
    "        \n",
    "\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        orig = gray.copy()\n",
    "        (origH, origW) = image.shape[:2]\n",
    " \n",
    "        (newW, newH) = (320,320)\n",
    "\n",
    "        rW = origW / float(newW)\n",
    "        rH = origH / float(newH)\n",
    "\n",
    "        image = cv2.resize(image, (newW, newH))\n",
    "        (H, W) = image.shape[:2]\n",
    "\n",
    "        blob = cv2.dnn.blobFromImage(image, 1.0, (W, H), (123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
    "        net = cv2.dnn.readNet(r'C:\\\\Users\\\\Abhilasha\\\\final_hackabit1\\\\frozen_east_text_detection.pb')\n",
    "\n",
    "        layerNames = [ \"feature_fusion/Conv_7/Sigmoid\",\"feature_fusion/concat_3\"]\n",
    "        net.setInput(blob)\n",
    "        (scores, geometry) = net.forward(layerNames)\n",
    "        def predictions(prob_score, geo):\n",
    "            (numR, numC) = prob_score.shape[2:4]\n",
    "            boxes = []\n",
    "            confidence_val = []\n",
    "            for y in range(0, numR):\n",
    "                scoresData = prob_score[0, 0, y]\n",
    "                x0 = geo[0, 0, y]\n",
    "                x1 = geo[0, 1, y]\n",
    "                x2 = geo[0, 2, y]\n",
    "                x3 = geo[0, 3, y]\n",
    "                anglesData = geo[0, 4, y]\n",
    "\n",
    "                for i in range(0, numC):\n",
    "                    if scoresData[i] < 0.9:\n",
    "                        continue\n",
    "                    (offX, offY) = (i * 4.0, y * 4.0)\n",
    "\n",
    "                    angle = anglesData[i]\n",
    "                    cos = np.cos(angle)\n",
    "                    sin = np.sin(angle)\n",
    "                    h = x0[i] + x2[i]\n",
    "                    w = x1[i] + x3[i]\n",
    "                    endX = int(offX + (cos * x1[i]) + (sin * x2[i]))\n",
    "                    endY = int(offY - (sin * x1[i]) + (cos * x2[i]))\n",
    "                    startX = int(endX - w)\n",
    "                    startY = int(endY - h)\n",
    "\n",
    "                    boxes.append((startX, startY, endX, endY))\n",
    "                    confidence_val.append(scoresData[i])\n",
    "\n",
    "            return (boxes, confidence_val)\n",
    "        (boxes, confidence_val) = predictions(scores, geometry)\n",
    "        boxes = non_max_suppression(np.array(boxes), probs=confidence_val)\n",
    "        results = []\n",
    "\n",
    "        for (startX, startY, endX, endY) in boxes:\n",
    "\n",
    "            startX = int(startX * rW)\n",
    "            startY = int(startY * rH)\n",
    "            endX = int(endX * rW)\n",
    "            endY = int(endY * rH)\n",
    "            r = orig[startY:endY, startX:endX] \n",
    "            configuration = (\"-l eng --oem 1 --psm 6\")\n",
    " \n",
    "            pytesseract.pytesseract.tesseract_cmd = r'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract'\n",
    "            text = pytesseract.image_to_string(r, config=configuration)\n",
    "            results.append(((startX, startY, endX, endY), text))\n",
    "   \n",
    "        orig_image = orig.copy()\n",
    "\n",
    "        for ((start_X, start_Y, end_X, end_Y), text) in results:\n",
    "            print(\"{}\\n\".format(text))\n",
    "            st.append(format(text))\n",
    "\n",
    "            text = \"\".join([x if ord(x) < 128 else \"\" for x in text]).strip()\n",
    "\n",
    "        list_1_string = ' '.join(st)\n",
    "        list_1_string = \" \".join(re.findall(r\"[a-zA-Z0-9]+\", list_1_string))\n",
    "        list_1_string = TextBlob(list_1_string.lower())\n",
    "        return render_template('index.html', message=list_1_string.correct())\n",
    "    \n",
    "    else:\n",
    "        message = \" Sorry you selected a wrong option\"\n",
    "        print(data)\n",
    "        return render_template('index.html', message=message)\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

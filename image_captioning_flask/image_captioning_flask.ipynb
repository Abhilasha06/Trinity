{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: Do not use the development server in a production environment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [19/Oct/2019 15:27:32] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [19/Oct/2019 15:27:33] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rashi.jpg\n",
      "C://Users//Abhilasha//Desktop//rashi.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Abhilasha\\Anaconda2\\envs\\tensorflow_env\\lib\\site-packages\\keras\\engine\\saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rashi.jpg\n",
      "Time taken in seconds = 4.904985189437866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [19/Oct/2019 15:30:30] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask\n",
    "from flask import request\n",
    "from flask import render_template\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "import pandas as pd\n",
    "import pickle   \n",
    "import string\n",
    "import os\n",
    "from PIL import Image\n",
    "import glob\n",
    "from pickle import dump, load\n",
    "from time import time\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                        Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.merge import add\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import Input, layers\n",
    "from keras import optimizers\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import cv2\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import save_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "app = Flask(__name__)\n",
    "@app.route('/')\n",
    "def my_form():\n",
    "    return render_template(\"my_form.html\")\n",
    "@app.route('/', methods=['POST'])\n",
    "def my_form_post():\n",
    "    st=[]\n",
    "    data1 = request.form['file']\n",
    "    print(data1) \n",
    "    imagepath = \"C://Users//Abhilasha//Desktop//\"+data1\n",
    "    print(imagepath)\n",
    "    image = cv2.imread(imagepath)\n",
    "    \n",
    "    def load_doc(filename):\n",
    "        file = open(filename, 'r')\n",
    "        text = file.read()\n",
    "        file.close()\n",
    "        return text\n",
    "    filename = \"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr8k.token.txt\"\n",
    "    doc = load_doc(filename)\n",
    "    def load_descriptions(doc):\n",
    "        mapping = dict()\n",
    "        for line in doc.split('\\n'):\n",
    "            tokens = line.split()\n",
    "            if len(line) < 2:\n",
    "                continue\n",
    "            image_id, image_desc = tokens[0], tokens[1:]\n",
    "            image_id = image_id.split('.')[0]\n",
    "            image_desc = ' '.join(image_desc)\n",
    "            if image_id not in mapping:\n",
    "                mapping[image_id] = list()\n",
    "            mapping[image_id].append(image_desc)\n",
    "        return mapping\n",
    "\n",
    "    descriptions = load_descriptions(doc)\n",
    "    def clean_descriptions(descriptions):\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        for key, desc_list in descriptions.items():\n",
    "            for i in range(len(desc_list)):\n",
    "                desc = desc_list[i]\n",
    "                desc = desc.split()\n",
    "                desc = [word.lower() for word in desc]\n",
    "                desc = [w.translate(table) for w in desc]\n",
    "                desc = [word for word in desc if len(word)>1]\n",
    "                desc = [word for word in desc if word.isalpha()]\n",
    "                desc_list[i] =  ' '.join(desc)\n",
    "\n",
    "    clean_descriptions(descriptions)\n",
    "    def to_vocabulary(descriptions):\n",
    "        all_desc = set()\n",
    "        for key in descriptions.keys():\n",
    "            [all_desc.update(d.split()) for d in descriptions[key]]\n",
    "        return all_desc\n",
    "\n",
    "    vocabulary = to_vocabulary(descriptions)\n",
    "    def save_descriptions(descriptions, filename):\n",
    "        lines = list()\n",
    "        for key, desc_list in descriptions.items():\n",
    "            for desc in desc_list:\n",
    "                lines.append(key + ' ' + desc)\n",
    "        data = '\\n'.join(lines)\n",
    "        file = open(filename, 'w')\n",
    "        file.write(data)\n",
    "        file.close()\n",
    "\n",
    "    save_descriptions(descriptions, 'descriptions.txt')\n",
    "    def load_set(filename):\n",
    "        doc = load_doc(filename)\n",
    "        dataset = list()\n",
    "        for line in doc.split('\\n'):\n",
    "            if len(line) < 1:\n",
    "                continue\n",
    "            identifier = line.split('.')[0]\n",
    "            dataset.append(identifier)\n",
    "        return set(dataset)\n",
    "\n",
    "    filename = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr_8k.trainImages.txt'\n",
    "    train = load_set(filename)\n",
    "    images = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Images\\\\'\n",
    "    img = glob.glob(images + '*.jpg')\n",
    "    train_images_file = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr_8k.trainImages.txt'\n",
    "    train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
    "    train_img = []\n",
    "\n",
    "    for i in img:\n",
    "        if i[len(images):] in train_images: \n",
    "            train_img.append(i) \n",
    "    test_images_file = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\Flickr_Data\\\\Flickr_TextData\\\\Flickr_8k.testImages.txt'\n",
    "    test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
    "    test_img = []\n",
    "\n",
    "    for i in img: \n",
    "        if i[len(images):] in test_images:\n",
    "            test_img.append(i) \n",
    "\n",
    "    def load_clean_descriptions(filename, dataset):\n",
    "\n",
    "        doc = load_doc(filename)\n",
    "        descriptions = dict()\n",
    "        for line in doc.split('\\n'):\n",
    "            tokens = line.split()\n",
    "            image_id, image_desc = tokens[0], tokens[1:]\n",
    "            if image_id in dataset:\n",
    "                if image_id not in descriptions:\n",
    "                    descriptions[image_id] = list()\n",
    "                desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "                descriptions[image_id].append(desc)\n",
    "        return descriptions\n",
    "\n",
    "    train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
    "    def preprocess(image_path):\n",
    "        img = load_img(image_path, target_size=(299, 299))\n",
    "        x = img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        return x\n",
    "\n",
    "    model = InceptionV3(weights='imagenet')\n",
    "    model_new = Model(model.input, model.layers[-2].output)\n",
    "    def encode(image):\n",
    "        image = preprocess(image) \n",
    "        fea_vec = model_new.predict(image) \n",
    "        fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) \n",
    "        return fea_vec\n",
    "    train_features = load(open(\"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\pickle\\\\encoded_train_images.pkl\", \"rb\"))\n",
    "    all_train_captions = []\n",
    "    for key, val in train_descriptions.items():\n",
    "        for cap in val:\n",
    "            all_train_captions.append(cap)\n",
    "    word_count_threshold = 10\n",
    "    word_counts = {}\n",
    "    nsents = 0\n",
    "    for sent in all_train_captions:\n",
    "        nsents += 1\n",
    "        for w in sent.split(' '):\n",
    "            word_counts[w] = word_counts.get(w, 0) + 1\n",
    "\n",
    "    vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
    "    ixtoword = {}\n",
    "    wordtoix = {}\n",
    "\n",
    "    ix = 1\n",
    "    for w in vocab:\n",
    "        wordtoix[w] = ix\n",
    "        ixtoword[ix] = w\n",
    "        ix += 1\n",
    "    vocab_size = len(ixtoword) + 1\n",
    "    def to_lines(descriptions):\n",
    "        all_desc = list()\n",
    "        for key in descriptions.keys():\n",
    "            [all_desc.append(d) for d in descriptions[key]]\n",
    "        return all_desc\n",
    "\n",
    "    def max_length(descriptions):\n",
    "        lines = to_lines(descriptions)\n",
    "        return max(len(d.split()) for d in lines)\n",
    "\n",
    "    max_length = max_length(train_descriptions)\n",
    "    def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        n=0\n",
    "        while 1:\n",
    "            for key, desc_list in descriptions.items():\n",
    "                n+=1\n",
    "                photo = photos[key+'.jpg']\n",
    "                for desc in desc_list:\n",
    "                    seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
    "                    for i in range(1, len(seq)):\n",
    "                        in_seq, out_seq = seq[:i], seq[i]\n",
    "                        in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "                        out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "                        X1.append(photo)\n",
    "                        X2.append(in_seq)\n",
    "                        y.append(out_seq)\n",
    "                if n==num_photos_per_batch:\n",
    "                    yield [[array(X1), array(X2)], array(y)]\n",
    "                    X1, X2, y = list(), list(), list()\n",
    "                    n=0\n",
    "    glove_dir = 'C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\glove6b200d'\n",
    "    embeddings_index = {}\n",
    "    f = open(os.path.join(glove_dir, 'glove.6B.200d.txt'), encoding=\"utf-8\")\n",
    "\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    \n",
    "    embedding_dim = 200\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    for word, i in wordtoix.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    from keras.models import load_model\n",
    "    model=load_model('C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\model_weights\\\\model_39.h5')\n",
    "    st=[]\n",
    "    data1 = request.form['file']\n",
    "    print(data1)\n",
    "    \n",
    "    image_p = \"C://Users//Abhilasha//Desktop//\"+data1\n",
    "   \n",
    "    start = time()\n",
    "    encoding_test = {}\n",
    "    encoding_test[image_p[len(images):]] = encode(image_p)\n",
    "    print(\"Time taken in seconds =\", time()-start)\n",
    "    with open(\"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\pickle\\\\encoded_test_image_single.pkl\", \"wb\") as encoded_pickle_single:\n",
    "        pickle.dump(encoding_test, encoded_pickle_single)\n",
    "    with open(\"C:\\\\Users\\\\Abhilasha\\\\image_captioning_flask\\\\pickle\\\\encoded_test_images.pkl\", \"rb\") as encoded_pickle_single:\n",
    "        encoding_test_single = load(encoded_pickle_single)\n",
    "    def greedySearch(photo):\n",
    "        in_text = 'startseq'\n",
    "        for i in range(max_length):\n",
    "            sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
    "            sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "            yhat = model.predict([photo,sequence], verbose=0)\n",
    "            yhat = np.argmax(yhat)\n",
    "            word = ixtoword[yhat]\n",
    "            in_text += ' ' + word\n",
    "            if word == 'endseq':\n",
    "                break\n",
    "        final = in_text.split()\n",
    "        final = final[1:-1]\n",
    "        final = ' '.join(final)\n",
    "        return final\n",
    "    \n",
    "    for i in range (0,1):\n",
    "        pic = list(encoding_test.keys())[i]\n",
    "        image = encoding_test[pic].reshape((1,2048))\n",
    "        ll=greedySearch(image)\n",
    "        return render_template('index.html', message=ll)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
